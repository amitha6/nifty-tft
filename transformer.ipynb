{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svl2ms8VT5I_"
      },
      "source": [
        "# Predicting NIFTY with Transformer\n",
        "\n",
        "The following is a Pytorch implementation of a transformer model to predict NIFTY indices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9AGD0eJTwf1"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7hWIQhXTqbV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "import torch\n",
        "\n",
        "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
        "from pytorch_forecasting.data import GroupNormalizer\n",
        "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
        "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZitYWIJTusu"
      },
      "source": [
        "## Data\n",
        "\n",
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMT1DJNyUIeO"
      },
      "outputs": [],
      "source": [
        "data = pd.read_excel('NIFTY50.xlsx', sheet_name=None)\n",
        "\n",
        "def copy_dict_without_keys(d, keys):\n",
        "    return {x: d[x] for x in d if x not in keys}\n",
        "\n",
        "input_data = copy_dict_without_keys(data, ['Companies List', 'in', 'Reverse REPO Rate', 'Repo Rate'])\n",
        "output_data = copy_dict_without_keys(data, list(set(data.keys()) - {'in'}))\n",
        "\n",
        "# impute null values in output data and convert date to standard format\n",
        "output_data = output_data.dropna(subset=['Open', 'High', 'Low', 'Close', 'Adj Close'])\n",
        "output_data['Date'] = pd.to_datetime(output_data['Date'], dayfirst=True)\n",
        "\n",
        "# prepare time index\n",
        "time_index = output_data['Date'].sort_values(ignore_index=True)\n",
        "time_index = pd.Series(time_index.index, time_index.values)\n",
        "\n",
        "# add time index to output data\n",
        "output_data['time_idx'] = pd.Series(list(map(time_index.get, output_data['Date'])))\n",
        "output_data = output_data.drop(['Date'], axis=1)\n",
        "\n",
        "# prepare input data before merge\n",
        "for sheet in input_data:\n",
        "    # 1. Replace '##...' by actual dates\n",
        "    # ('##...' are placeholders for wider text)\n",
        "\n",
        "    # 2. Impute null values \n",
        "    input_data[sheet] = data[sheet].dropna(subset=['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'])\n",
        "\n",
        "    # 3. Add a categorical column for company symbol\n",
        "    input_data[sheet]['Symbol'] = sheet\n",
        "\n",
        "    # 4. Convert date to standard format\n",
        "    input_data[sheet]['Date'] = pd.to_datetime(input_data[sheet]['Date'], dayfirst=True)\n",
        "\n",
        "    # 5. Add time index as per time origin (of whole dataset)\n",
        "    input_data[sheet]['time_idx'] = pd.Series(list(map(time_index.get, input_data[sheet]['Date']))) \n",
        "    input_data[sheet] = input_data[sheet].drop(['Date'], axis=1)\n",
        "\n",
        "# map every timestep to its list of companies\n",
        "# for sheet in input_data:\n",
        "#     output_data[sheet] = output_data['time_idx'].isin(input_data[sheet]['time_idx'])\n",
        "\n",
        "# store list of company symbols\n",
        "companies = input_data.columns.values.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Merge Data\n",
        "\n",
        "Since every target value is mapped to by a 2D vector, it is necessary to have in the training data a column of 2D vectors mapped to a time step.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Concatenate dataframes\n",
        "input_data = pd.concat(input_data)\n",
        "\n",
        "# 2. Collect event data from separate columns into vectors\n",
        "input_data['events'] = input_data['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'].values.tolist()\n",
        "input_data['events'] = input_data['events'].apply(np.array)\n",
        "input_data = input_data[['time_idx', 'events', 'Symbol']]\n",
        "\n",
        "# 3. Group by time step (as we need every time step sample to map to a single target value)\n",
        "merged_data = input_data.groupby('time_idx')['events'].apply(list).reset_index()\n",
        "merged_data['events'] = merged_data['events'].apply(np.array)\n",
        "company_categories = input_data.groupby('time_idx')['Symbol'].apply(list).reset_index()\n",
        "merged_data = pd.merge(merged_data, company_categories, how='inner', on='time_idx')\n",
        "    \n",
        "# 4. Sort by time step before concatenation with NIFTY data\n",
        "merged_data = merged_data.sort_values(by=['time_idx'], ignore_index=True)\n",
        "\n",
        "# 5. Add columns for NIFTY index\n",
        "merged_data = pd.merge(merged_data, output_data, how='inner', on='time_idx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIt1kUqvuALf"
      },
      "source": [
        "## Create Dataset and dataloaders\n",
        "\n",
        "Convert the dataframe of raw data into a PyTorch Forecasting ``TimeSeriesDataSet``. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbxAl2ZCt_j_"
      },
      "outputs": [],
      "source": [
        "training = TimeSeriesDataSet(\n",
        "    data=merged_data,\n",
        "    time_idx='time_idx',\n",
        "    target='Close',\n",
        "    group_ids=['Symbol'],\n",
        "    min_encoder_length=16,  # keep encoder length long (as it is in the validation set)\n",
        "    max_encoder_length=64,\n",
        "    static_categoricals=[], # entity embedding is used for categorical variables, instead of one-hot\n",
        "    time_varying_known_categoricals=[],\n",
        "    time_varying_known_reals=['time_idx'],\n",
        "    time_varying_unknown_reals=['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'],\n",
        "    time_varying_unknown_categoricals=['Symbol'],\n",
        "    target_normalizer=GroupNormalizer(\n",
        "        groups=[], transformation='softplus'\n",
        "    )  # use softplus and normalize by group\n",
        ")\n",
        "\n",
        "# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
        "# for each series\n",
        "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
        "\n",
        "# create dataloaders for model\n",
        "batch_size = 128\n",
        "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
        "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLM6Ua0Wu94c"
      },
      "source": [
        "## Transformer\n",
        "\n",
        "Initialize and train a time-series forecasting transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAft7MUiyY2_"
      },
      "source": [
        "### Optimize learning rate\n",
        "\n",
        "The optimal learning rate is identified using PyTorch Lightning learning rate finder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHxkjRCJyXMh"
      },
      "outputs": [],
      "source": [
        "# configure network and trainer\n",
        "pl.seed_everything(42)\n",
        "trainer = pl.Trainer(\n",
        "    gpus=0,\n",
        "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
        "    # of the gradient for recurrent neural networks\n",
        "    gradient_clip_val=0.1,\n",
        ")\n",
        "\n",
        "\n",
        "tft = TemporalFusionTransformer.from_dataset(\n",
        "    training,\n",
        "    # not meaningful for finding the learning rate but otherwise very important\n",
        "    learning_rate=0.03,\n",
        "    hidden_size=16,  # most important hyperparameter apart from learning rate\n",
        "    # number of attention heads. Set to up to 4 for large datasets\n",
        "    attention_head_size=2,\n",
        "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
        "    hidden_continuous_size=8,  # set to <= hidden_size\n",
        "    output_size=7,  # 7 quantiles by default\n",
        "    loss=QuantileLoss(),\n",
        "    # reduce learning rate if no improvement in validation loss after x epochs\n",
        "    reduce_on_plateau_patience=4,\n",
        ")\n",
        "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cm0jEK5qy2r8"
      },
      "outputs": [],
      "source": [
        "# find optimal learning rate\n",
        "res = trainer.tuner.lr_find(\n",
        "    tft,\n",
        "    train_dataloaders=train_dataloader,\n",
        "    val_dataloaders=val_dataloader,\n",
        "    max_lr=10.0,\n",
        "    min_lr=1e-6,\n",
        ")\n",
        "\n",
        "print(f\"suggested learning rate: {res.suggestion()}\")\n",
        "fig = res.plot(show=True, suggest=True)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7uFOj6IzDeh"
      },
      "source": [
        "For the TemporalFusionTransformer, the optimal learning rate seems to be slightly lower than the suggested one. Further, we do not directly want to use the suggested learning rate because PyTorch Lightning sometimes can get confused by the noise at lower learning rates and suggests rates far too low. Manual control is essential."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYTqF5WAzKAA"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLeBIp96zZjg"
      },
      "outputs": [],
      "source": [
        "# configure network and trainer\n",
        "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
        "lr_logger = LearningRateMonitor()  # log the learning rate\n",
        "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=30,\n",
        "    gpus=0,\n",
        "    weights_summary=\"top\",\n",
        "    gradient_clip_val=0.1,\n",
        "    limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
        "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
        "    callbacks=[lr_logger, early_stop_callback],\n",
        "    logger=logger,\n",
        ")\n",
        "\n",
        "\n",
        "tft = TemporalFusionTransformer.from_dataset(\n",
        "    training,\n",
        "    learning_rate=0.03,\n",
        "    hidden_size=16,\n",
        "    attention_head_size=1,\n",
        "    dropout=0.1,\n",
        "    hidden_continuous_size=8,\n",
        "    output_size=7,  # 7 quantiles by default\n",
        "    loss=QuantileLoss(),\n",
        "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
        "    reduce_on_plateau_patience=4,\n",
        ")\n",
        "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ej3S9dlC3q5I"
      },
      "outputs": [],
      "source": [
        "# for training with gpu\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "with tf.device(\"gpu\"):\n",
        "    trainer.fit(\n",
        "        tft,\n",
        "        train_dataloaders=train_dataloader,\n",
        "        val_dataloaders=val_dataloader,\n",
        "    )\n",
        "\n",
        "# for training with cpu\n",
        "\n",
        "# trainer.fit(\n",
        "#     tft,\n",
        "#     train_dataloaders=train_dataloader,\n",
        "#     val_dataloaders=val_dataloader,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwQug2mZ39pn"
      },
      "outputs": [],
      "source": [
        "best_model_path = trainer.checkpoint_callback.best_model_path\n",
        "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calcualte mean absolute error on validation set\n",
        "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
        "predictions = best_tft.predict(val_dataloader)\n",
        "(actuals - predictions).abs().mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
        "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
        "\n",
        "for idx in time_index:  # plot \n",
        "    best_tft.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True);"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
